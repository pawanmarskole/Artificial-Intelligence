{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yashad Samant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Balancing the robot on two wheels efficiently is a task. While PID controllers are used to tackle this problem, we cannnot rely completely on it because it's a bit inaccurate and there's a lot of estimation has to be done through trial and error run manually which adds on to the inconvenience. Moreover, we cannot use PID to adapt to the environment because there is no machine learning involved. Thus, it is necessary to find an efficient solution to this problem which is also adaptable to the environment. As we know, in a real world environment, while considering a robot we don't really have a target to get at. But we have a small idea about the environment, thus reinforcement learning seems to be a good method for this problem. \n",
    "\n",
    "I have read a couple of papers tackling this problem. There are few github repositories which touch this topic as well. I am going to reference these, to get to my goal. References are mentioned below.\n",
    "\n",
    "I have solely chosen this project to increase my understanding of the algorithm and learn more about it and be comfortable in using it wherever required.\n",
    "\n",
    "Reinforcement learning has both the qualities of supervised and unsupervised learning but with not very accurate data, it produces output precisely in far lesser time than unsupervised learning. Personally, I haven't implemented a real life problem using this method, hence, I want to evaluate the prowess of this algorithm and consequently learn more about it.\n",
    "\n",
    "I am working alone on this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Problem\n",
    "\n",
    "Aim is to develop a two wheeled system which can balance on its own. Project is based on http://ieeexplore.ieee.org/document/4667312/ which incorporates the use of Fuzzy neural networks and reinforcement learning to achieve the objective. \n",
    "My objectives are as follows -:\n",
    "\n",
    "* Develop a system on Pi 3 and obtain values from MPU 6050 (3-axis accelerometer & 3-axis gyroscope).\n",
    "* Combine the raw values from 3-axis accelerometer and 3-axis gyroscope separately using fuzzy neural network. Thus, this network will have 6 inputs and 2 outputs.\n",
    "* Based on the two outputs obtained from the fuzzy system, develop a reinforcement learning algorithm to train the Q values and find subsequent states.\n",
    "* Plot necessary data related to the algorithms and generate a document.\n",
    "\n",
    "** Now, objective can be obtained using only fuzzy neural networks but I want to check how reinforcement learning develops the efficiency of the code **.\n",
    "\n",
    "**NOTE - Pi is only used to obtain values from the sensors and save it. Programming will be done on desktop/laptop**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "### Fuzzy Neural Network\n",
    "\n",
    "#### Fuzzy Logic\n",
    "Fuzzy logic is a form of many-valued logic in which the truth values of variables may be any real number between 0 and 1. By contrast, in Boolean logic, the truth values of variables may only be the integer values 0 or 1. Fuzzy logic has been employed to handle the concept of partial truth, where the truth value may range between completely true and completely false. Furthermore, when linguistic variables are used, these degrees may be managed by specific (membership) functions.\n",
    "\n",
    "Since the fuzzy system output is a consensus of all of the inputs and all of the rules, fuzzy logic systems can be well behaved when input values are not available or are not trustworthy. Weightings can be optionally added to each rule in the rulebase and weightings can be used to regulate the degree to which a rule affects the output values. These rule weightings can be based upon the priority, reliability or consistency of each rule. These rule weightings may be static or can be changed dynamically, even based upon the output from other rules.\n",
    "\n",
    "https://en.wikipedia.org/wiki/File:Fuzzy_logic_temperature_en.svg\n",
    "\n",
    "\n",
    "Fuzzify all input values into fuzzy membership functions.\n",
    "Execute all applicable rules in the rulebase to compute the fuzzy output functions.\n",
    "De-fuzzify the fuzzy output functions to get \"crisp\" output values.\n",
    "\n",
    "#### Adaptive Fuzzy Logic\n",
    "\n",
    "http://www.scholarpedia.org/article/File:Cooperative_fnn.png\n",
    "\n",
    "A fuzzy neural network or neuro-fuzzy system is a learning machine that finds the parameters of a fuzzy system (i.e., fuzzy sets, fuzzy rules) by exploiting approximation techniques from neural networks.\n",
    "\n",
    "A neuro-fuzzy system based on an underlying fuzzy system is trained by means of a data-driven learning method derived from neural network theory. This heuristic only takes into account local information to cause local changes in the fundamental fuzzy system.\n",
    "It can be represented as a set of fuzzy rules at any time of the learning process, i.e., before, during and after.\n",
    "Thus the system might be initialized with or without prior knowledge in terms of fuzzy rules.\n",
    "The learning procedure is constrained to ensure the semantic properties of the underlying fuzzy system.\n",
    "A neuro-fuzzy system approximates a n-dimensional unknown function which is partly represented by training examples.\n",
    "Fuzzy rules can thus be interpreted as vague prototypes of the training data.\n",
    "A neuro-fuzzy system is represented as special three-layer feedforward neural network as it is shown in Figure 1.\n",
    "* The first layer corresponds to the input variables.\n",
    "* The second layer symbolizes the fuzzy rules.\n",
    "* The third layer represents the output variables.\n",
    "\n",
    "\n",
    "The fuzzy sets are converted as (fuzzy) connection weights.\n",
    "Some approaches also use five layers where the fuzzy sets are encoded in the units of the second and fourth layer, respectively. However, these models can be transformed into a three-layer architecture.\n",
    "One can basically distinguish between three different kinds of fuzzy neural networks, i.e., cooperative, concurrent and hybrid FNNs\n",
    "\n",
    "### Reinforcement Learning\n",
    "\n",
    "Reinforcement learning (RL) is an area of machine learning inspired by behaviourist psychology, concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. The problem, due to its generality, is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In the operations research and control literature, the field where reinforcement learning methods are studied is called approximate dynamic programming. The problem has been studied in the theory of optimal control, though most studies are concerned with the existence of optimal solutions and their characterization, and not with the learning or approximation aspects. In economics and game theory, reinforcement learning may be used to explain how equilibrium may arise under bounded rationality.\n",
    "\n",
    "In machine learning, the environment is typically formulated as a Markov decision process (MDP), as many reinforcement learning algorithms for this context utilize dynamic programming techniques.[1] The main difference between the classical techniques and reinforcement learning algorithms is that the latter do not need knowledge about the MDP and they target large MDPs where exact methods become infeasible.\n",
    "\n",
    "https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Reinforcement_learning_diagram.svg/1059px-Reinforcement_learning_diagram.svg.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeline\n",
    "\n",
    "Week 1 - Reading http://ieeexplore.ieee.org/document/4667312/ paper thoroughly to get insight of the topic. Interfacing MPU 6050 with raspberry pi and taking values from the sensor depending upon different orientations.\n",
    "\n",
    "Week 2  - Create tabular data and apply fuzzy logic to the raw sensor values and train the neural network accordingly.\n",
    "\n",
    "Week 3 & 4- Instigate Q value training on the fuzzified output as mentioned in the paper.\n",
    "\n",
    "Week 5 - Completing the Jupyter notebook and submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "http://ieeexplore.ieee.org/document/7858451/?reload=true\n",
    "\n",
    "http://ieeexplore.ieee.org/document/4667312/\n",
    "\n",
    "https://en.wikipedia.org/wiki/Reinforcement_learning\n",
    "\n",
    "http://www.scholarpedia.org/article/Fuzzy_neural_network\n",
    "\n",
    "https://github.com/famalgosner/roboticsControl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
